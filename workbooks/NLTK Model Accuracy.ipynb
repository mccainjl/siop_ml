{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing Model Accuracy\n",
    "\n",
    "The purpose of this notebook is to build on an example notebook, and add test holdouts and to show model evaluation.\n",
    "\n",
    "At the end of this notebook, we can see how much better or worse than guessing the mean for input would be.  For this basic model, it performs roughly at the level of guessing the mean.  This notebook is uesful for demonstrating the use of train_test_split, which can be used for setting aside a test set in Python, and for the use of a `.score()` method, which exists for most models to demonstrate the accuracy against a holdout set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open_ended_2</th>\n",
       "      <th>C_Scale_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I would reach out to my boss and ask him or he...</td>\n",
       "      <td>3.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I would continue to work on the project that w...</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I would start working on the project now and g...</td>\n",
       "      <td>4.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        open_ended_2  C_Scale_score\n",
       "0  I would reach out to my boss and ask him or he...       3.750000\n",
       "1  I would continue to work on the project that w...       5.000000\n",
       "2  I would start working on the project now and g...       4.666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data to a df\n",
    "df = pd.read_csv('data/siop_ml_train_participant.csv')\n",
    "\n",
    "# Limit the results to a single answer / score combination for demonstration\n",
    "df = df.drop(['Respondent_ID', 'open_ended_1', 'open_ended_3', 'open_ended_4', 'open_ended_5', \n",
    "         'A_Scale_score', 'O_Scale_score', 'E_Scale_score', 'N_Scale_score'], axis=1)\n",
    "\n",
    "# Confirm that the data has been imported and is formatted correctly\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'I': 4, 'my': 3, 'boss': 2, 'to': 2, 'it': 2, 'this': 2, 'would': 2, 'results.': 1, 'gonna': 1, 'what': 1, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "tokens = [t for t in df['open_ended_2'][0].split()]\n",
    "nltk.FreqDist(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these two to set which facet you're attempting to predict from which input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_columns = ['open_ended_1', 'open_ended_2', 'open_ended_3', 'open_ended_4', 'open_ended_5']\n",
    "criterion_columns = ['E_Scale_score', 'A_Scale_score', 'O_Scale_score', 'C_Scale_score', 'N_Scale_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_prep (df):\n",
    "    for col in training_columns:\n",
    "        \n",
    "        # Lowercase it all\n",
    "        df[col].str.lower()\n",
    "    \n",
    "        # Remove non-alphanumeric characters\n",
    "        df[col].replace('[^a-zA-Z0-9]', ' ', regex = True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "prepped_data = simple_prep (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(761, 11)\n",
      "(327, 11)\n"
     ]
    }
   ],
   "source": [
    "# Generate our testing and training sets and show their relative sizes\n",
    "train, test = train_test_split(prepped_data, test_size=0.3)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When hosted in GitHub, this sample will only show the most recent run, however you can clone this repository to run it locally and sample the results yourself.  The results for this model are not great, but better than guessing.  Standard deviation is provided for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = 'open_ended_2'\n",
    "regressor = 'C_Scale_score'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10', '100', '30', '40', '45', '8n', '90', '95', 'abandon', 'abilities']\n"
     ]
    }
   ],
   "source": [
    "def vectorize_data (df_train, df_test, predictor, regressor):\n",
    "    # Set the TF-IDF vectorization settings\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    \n",
    "    # Convert text into vectors\n",
    "    X_train = vectorizer.fit_transform(df_train[predictor]) \n",
    "\n",
    "    # Also convert test data into vectors\n",
    "    X_test = vectorizer.transform(df_test[predictor]) \n",
    "    \n",
    "    # Set the criterion values for the training set\n",
    "    y_train = df_train[regressor]\n",
    "\n",
    "    # Set the criterion values for the test set\n",
    "    y_test = df_test[regressor]\n",
    "    \n",
    "    # return the vectorizer object so we can use it later for evaluation\n",
    "    return X_train, X_test, y_train, y_test, vectorizer\n",
    "  \n",
    "X_train, X_test, y_train, y_test, vectorizer = vectorize_data (train, test, predictor, regressor)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "print(feature_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for pred C_Scale_score from open_ended_2 is -0.021640579428152007\n"
     ]
    }
   ],
   "source": [
    "# Utilizing ridge regression\n",
    "mod = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "# Fit the model using the our training data and criterion column\n",
    "mod.fit(X_train, y_train)\n",
    "\n",
    "print('Score for pred {0} from {1} is {2}'.format(regressor, predictor, mod.score(X_test, y_test)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
